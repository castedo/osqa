{% extends 'osa/_answer.md.jinja' %}
{% set aid = 139 %}
{% set answers_questions = [1036] %}
{% set title = "Sample vs population variance with multivariate distributions" %}

{% block answer %}

{% raw %}
\newcommand{\MSE}[1]{\operatorname{MSE}\!\left[{ #1}\right]}
{% endraw %}

>
> ## WORK IN PROGRESS
>

# Introduction

On a normal distribution, sample variance does not perform better than
population variance, as a point estimator, evaluated by Mean Squared Error
(MSE).  Similarly, on a Bernoulli distribution, sample variance {{alink('does
not perform better than population variance ', 138)}}, as a point estimator,
evaluated by MSE and Probability of Impossible Estimand [@castedo_osa_138].

However, this underperformance of sample variance does not
extend to the sum of variances of a multivariate distribution.
Depending on the multivariate distribution, below a certain number of samples
and above a certain number of variables (or dimensions), sample variance
achieves lower MSE than population variance. This document provides 
simple conditions for when lower MSE is achieved.

# Conditions favoring sample variance

A $d$-dimensional random vector $D = (D_1, D_2, ..., D_d)$ specifies a
multivariate distribution.  We assume the component distributions are
independent, but they need not be identically distributed.  Each $D_c$ can be
any distribution with finite mean and variance.

The population parameter to be estimated is the sum of variances to be denoted:
\begin{eqnarray*}
\theta & = & \sum_{c=1}^d \Vars{D_c}
\end{eqnarray*}

Let $S_c$ denote the population variance of $n$ samples
from component distribution $D_c$.
$$
S_c := \frac{1}{n}
    \sum_{i=1}^n \left(
       X_{i,c} -  \frac{1}{n} \sum_{j=1}^n X_{j,c} 
    \right)^2
$$
where $X_i$ are i.i.d. samples.

Let $S$ denote the sum of population variances of the components.
$$
S := \sum_{c=1}^d S_c
$$

The expectation of population variance [@degroot_probability_2002] at component
$c$ is
\begin{eqnarray*}
\Es{S_c} & = & \frac{n-1}{n} \Vars{D_c}
\end{eqnarray*}
and thus the expectation of the sum of population variances is
\begin{eqnarray*}
\Es{S} & = & \frac{n-1}{n} \theta
\end{eqnarray*}

It will be convenient to look at the expectation and variance of population
variance averaged across components.

Let $s_*$ equal the expected estimate averaged across all dimensions.
\begin{eqnarray*}
  s_* & := & \frac{1}{d} \sum_{c=1}^d \Es{S_c}
\end{eqnarray*}
and $v_*$ equal the estimator variance averaged across all dimensions.
\begin{eqnarray*}
  v_* & := & \frac{1}{d} \sum_{c=1}^d \Vars{S_c} 
\end{eqnarray*}

The expected value and variance of the sum of population variances can be
expressed in terms of these averages.
$$
\frac{\Es{S}^2}{\Vars{S}} 
= \frac{\left(\sum_{c=1}^d \Es{S_c}\right)^2}{\sum_{c=1}^d \Vars{S_c}}
= \frac{s_*^2}{v_*} d
$$

Thus by Theorem 1 the sum of samples variances has lower MSE than the sum of
population
variances if and only if
\begin{eqnarray*}
\frac{s_*^2}{v_*} d & > & 2n-1
\end{eqnarray*}

This result applies to any distribution as long as the averages $s_*$ and $v_*$
exist. We can see that for a large numbers of dimensions and not particularly
small average estimates, the condition is met for sample variance to outperform
population variance.

More precise conditions can be isolation assume particular distributions such
as Bernoulli and normal distributions.


## Multivariate Bernoulli distributions

With Theorem 0 the following simple condition
\begin{eqnarray*}
\sum_{c=1}^d p_c  & \ge & n + 1  \\
\end{eqnarray*}
implies the sample variance achieves lower MSE than population variance.
The $p_c$ are the probabilities of the Bernoulli distributions, but after
flipping any Bernoulli distributions to have probability less than $1/2$.
In other words, if any distribution $B_c$ has probability
$p_c > 1/2$, the corresponding random variable $B_c$ can be replaced
by a flipped random variable $B'_c := 1 - B_c$.


## Multivariate normal distributions

Consider any $S_c$ as the population variance of $n$ samples from a normal
distribution with variance $\sigma^2$.
$n S_c / \sigma^2$ has a chi-squared distribution with $n-1$ degrees of freedom
[@degroot_probability_2002] and thus:
\begin{eqnarray*}
\Es{n S_c / \sigma^2} & = & n-1 \\
\Es{S_c} & = & \frac{n-1}{n \sigma^2}  \\
\Vars{n S_c / \sigma^2} & = & 2(n-1) \\
\Vars{S_c} & = & \frac{2(n-1)}{n^2 \sigma^4} \\
\frac{\Es{S_c}^2}{\Vars{S_c}} & = & \frac{n-1}{2} \\
\end{eqnarray*}

For a multivariate normal distribution,
if all $\Es{S_c}$ are close to $s_*$ and all $\Vars{S_c}$ are close to $v_*$,
then
\begin{eqnarray*}
\frac{n-1}{2} d & \approx & \frac{s_*^2}{v_*} d
\end{eqnarray*}
in which case
\begin{eqnarray*}
\frac{n-1}{2} d & \gtrapprox & 2n-1
\end{eqnarray*}
which further simplifies to
\begin{eqnarray*}
d & \gtrapprox & 2 \left( \frac{n}{n-1} + \frac{n-1}{n-1} \right)  \\
d & \gtrapprox & 4 + \frac{2}{n-1}  \\
\end{eqnarray*}
will approximate the boundary between sample variance vs population 
variance having lower or higher variance.


# Conclusion

As long as these exists a finite average across dimensions of variance to be
estimated and variance of the estimates, there is some number of dimensions
large enough for the sum of sample variances to be a better estimator than the
sum of population variances, as measured by MSE.

In the general case, Theorem 0 provides a simple condition based on
averages across dimensions. In the case of multivariate Bernoulli distributions,
if the sum of distribution probabilities is greater than $n+1$, then sample
variance performs better.
For multivariate normal distributions whose variates are independent and
_approximately_ identical, sample variance will achieve lower MSE after $4$
dimensions.


# Proofs

#### Theorem 0

Given $n \ge 2$ independent samples from a multivariate distribution of $d$
jointly independent Bernoulli distributions with probabilities $p_c \le 1/2$
for $c \in \{1, ..., d\}$, if
\begin{eqnarray*}
\sum_{c=1}^d p_c  & \ge & n + 1  \\
\end{eqnarray*}
then the sum of sample variances has lower MSE than the sum of population
variances.

**Proof**
\begin{eqnarray*}
1 + \frac{1}{n-1}
  & = &
\frac{n}{n-1}  \\
2n + 2\left(1 + \frac{1}{n-1}\right)
  & = &
2n + 2 \frac{n}{n-1}  \\
2n + \left(1 + \frac{1}{n-1}\right)
  & = &
2n \left(1 + \frac{1}{n-1}\right) - \left(1 + \frac{1}{n-1}\right)  \\
2n + 1 + \frac{1}{n-1}
  & = &
\frac{n}{n-1}(2n-1)  \\
\end{eqnarray*}

Let
\begin{eqnarray*}
p_* & := & \frac{1}{d} \sum_{c=1}^d p_c \\
\end{eqnarray*}

\begin{eqnarray*}
\sum_{c=1}^d p_c  & \ge & n + 1  \\
2 p_* d  & \ge & 2(n + 1)  \\
  & \ge & 2n + 1 + \frac{1}{n-1}  \\
  & \ge & \frac{n}{n-1} (2n - 1)  \\
2 p_* d \frac{n-1}{n} & \ge & 2n - 1  \\
\end{eqnarray*}

By Theorem 2,
\begin{eqnarray*}
\frac{s_*^2}{v_*} d & \ge & 2 p_* d \frac{n-1}{n}  \\
  & \ge & 2n - 1  \\
\end{eqnarray*}

Given that
$$
\frac{\Es{S}^2}{\Vars{S}} 
= \frac{\left(\sum_{c=1}^d \Es{S_c}\right)^2}{\sum_{c=1}^d \Vars{S_c}}
= \frac{\left(s_* d\right)^2}{v_* d}
= \frac{s_*^2}{v_*} d
$$

From Theorem 1, it follow that the sum of sample variances has smaller MSE than
the sum of population variances.


#### Theorem 1

Consider any estimator $F$ of parameter $\theta$ from a specific distribution where
\begin{eqnarray*}
  \Es{F} & = & \frac{n-1}{n} \theta
\end{eqnarray*}
It follows that
\begin{eqnarray*}
  \MSE{F} & > & \MSE{\frac{n}{n-1} F}
\end{eqnarray*}
if and only if
\begin{eqnarray*}
  \frac{\Es{F}^2}{\Vars{F}} & > & 2n-1
\end{eqnarray*}

**Proof**

\begin{eqnarray*}
\Es{F} & = & \frac{n-1}{n} \theta  \\
\left( 1 + \frac{1}{n-1} \right) \Es{F} & = & \theta  \\
\Es{F - \theta} & = & - \frac{\Es{F}}{n-1}   \\
\end{eqnarray*}

\begin{eqnarray*}
\MSE{F} & > & \MSE{\frac{n}{n-1} F}  \\
\Vars{F} + \Es{F-\theta}^2 & > & \Vars{\frac{n}{n-1}F} + 0^2  \\
\Es{F-\theta}^2 & > & \left(\frac{n^2}{(n-1)^2} - 1 \right) \Vars{F} \\
\left(\frac{\Es{F}}{n-1}\right)^2 & > & \frac{2n-1}{(n-1)^2} \Vars{F} \\
\frac{\Es{F}^2}{\Vars{F}} & > & 2n-1  \\
\end{eqnarray*}
QED


#### Theorem 2

Given independent samples $B_1$, ..., $B_n$ from a multivariate distribution
of $d$ Bernoulli distributions with probabilities $p_c \le 1/2$
for $c \in \{1, ..., d\}$,
\begin{eqnarray*}
\frac{s_*^2}{v_*} & \ge & 2 p_* \frac{n-1}{n}  \\
\end{eqnarray*}
where
$p_*$, $s_*$, and $v_*$ denote the average across component

* Bernoulli distribution probability,
* estimator expectation of population variance, 
* estimator variance of population variance,

respectively. More formally,
\begin{eqnarray*}
p_* & := & \frac{1}{d} \sum_{c=1}^d p_c \\
s_* & := & \frac{1}{d} \sum_{c=1}^d \Es{S_c}  \\
v_* & := & \frac{1}{d} \sum_{c=1}^d \Vars{S_c}  \\
S_c & := & \hat{p}_c (1-\hat{p}_c)  \\
\hat{p}_c & := & \frac{1}{n} \sum_{i=1}^n B_{i,c}  \\
\end{eqnarray*}
where $B_i = (B_{i,1}, B_{i,2}, ..., B_{i,d})$ is the $i$-th sample from the
$d$-dimensional multivariate distribution.  $S_c$ is the population variance of
the $n$ samples at the $c$-th component variable.

**Proof**

By Lemma 1, it is accurate to call $\hat{p}_c (1-\hat{p}_c)$ population variance.

By Lemma 2,
\begin{eqnarray*}
\frac{1}{d} \sum_{c=1}^d \frac{\Es{S_c}}{4}
   & \ge & \frac{1}{d} \sum_{c=1}^d \Vars{S_c}  \\
\frac{1}{4} s_* & \ge & v_*   \\
\frac{s_*^2}{v_*} & \ge & 4 s_*  \\
\end{eqnarray*}

From the expectation of population variance, the variance of a Bernoulli
distributions, and $p_c \le 1/2$, for all $c$ we have
\begin{eqnarray*}
\Es{S_c}
  & = & \frac{n-1}{n} p(1-p)  \\
  & \ge & \frac{n-1}{n} \frac{p_c}{2}  \\
\end{eqnarray*}

Combining results gets
\begin{eqnarray*}
\frac{1}{d} \sum_{c=1}^d \Es{S_c}
  & \ge & \frac{1}{d} \sum_{c=1}^d \frac{n-1}{n} \frac{p_c}{2}  \\
s_* & \ge & \frac{n-1}{2n} p_*  \\
4 s_* & \ge & 2 \frac{n-1}{n} p_*  \\
\frac{s_*^2}{v_*} & \ge & 2 p_* \frac{n-1}{n}  \\
\end{eqnarray*}


#### Lemma 1

$\hat{p} (1-\hat{p})$ equals the population variance of samples $B_1$, ...,
$B_n$ from a Bernoulli distribution (taking values $0$ or $1$) where
\begin{eqnarray*}
\hat{p} & := & \frac{1}{n} \sum_{i=1}^n B_i
\end{eqnarray*}

**Proof**

\begin{eqnarray*}
\frac{1}{n} \sum_{i=1}^n \left(B_i - \hat{p} \right)^2
  & = & \frac{1}{n} \sum_{i=1}^n B_i^2
        - 2 \hat{p}_c\frac{1}{n} \sum_{i=1}^n B_i + \hat{p}^2  \\
  & = & \frac{1}{n} \sum_{i=1}^n B_i - \hat{p}^2  \\
  & = & \hat{p} - \hat{p}^2  \\
  & = & \hat{p} (1-\hat{p})  \\
\end{eqnarray*}
QED


#### Lemma 2

Let $S$ denote population variance of samples drawn from a Bernoulli distribution.
\begin{eqnarray*}
\Vars{S} & \le & \frac{\Es{S}}{4}  \\
\end{eqnarray*}

**Proof**

Define $\hat{p}$ as in Lemma 1.  Since $\hat{p} \in [0, 1]$, the following
inequalities must hold.
\begin{eqnarray*}
\hat{p} (1-\hat{p}) & \le & \frac{1}{4}  \\
\hat{p}^2 (1-\hat{p})^2 & \le & \frac{\hat{p} (1-\hat{p})}{4}  \\
\Es{S^2} & \le & \frac{\Es{S}}{4}  \\
\Es{S^2} - \Es{S}^2
  & \le & \Es{S} \left(\frac{1}{4} - \Es{S}\right)  \\
\Vars{S} & \le & \frac{\Es{S}}{4}  \\
\end{eqnarray*}
QED


# References

{% endblock answer %}

